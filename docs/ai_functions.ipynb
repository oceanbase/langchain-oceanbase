{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: AI Functions\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OceanBase AI Functions\n",
    "\n",
    "This notebook covers how to use OceanBase AI functions, including AI_EMBED, AI_COMPLETE, and AI_RERANK functions available in OceanBase 4.4.1+ and SeekDB.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Setup](#setup) - Deploy OceanBase and install dependencies\n",
    "- [Initialization](#initialization) - Configure and create AI functions client\n",
    "- [Model Configuration Steps](#model-configuration-steps) - Step-by-step model and endpoint configuration\n",
    "- [Test AI Functions](#test-ai-functions) - Test AI_COMPLETE, AI_EMBED, and AI_RERANK\n",
    "- [AI_EMBED](#ai_embed) - Convert text to vector embeddings\n",
    "- [AI_COMPLETE](#ai_complete) - Generate text using LLM\n",
    "- [AI_RERANK](#ai_rerank) - Rerank search results for better accuracy\n",
    "- [Batch Operations](#batch-operations) - Process multiple texts efficiently\n",
    "- [Use Cases](#use-cases) - Real-world application examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To use OceanBase AI functions, you'll need to deploy OceanBase 4.4.1+ or SeekDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --name=oceanbase -e MODE=mini -e OB_SERVER_IP=127.0.0.1 -p 2881:2881 -d oceanbase/oceanbase-ce:4.4.1.0-100000032025101610\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And install the `langchain-oceanbase` integration package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU \"langchain-oceanbase\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the connection to OceanBase and set the memory usage ratio for vector data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyobvector import ObVecClient\n",
    "\n",
    "tmp_client = ObVecClient()\n",
    "tmp_client.perform_raw_text_sql(\"ALTER SYSTEM ob_vector_memory_limit_percentage = 30\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Configure the connection parameters and initialize the OceanBase AI functions client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_oceanbase.ai_functions import OceanBaseAIFunctions\n",
    "\n",
    "connection_args = {\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": \"2881\",\n",
    "    \"user\": \"root@test\",\n",
    "    \"password\": \"\",\n",
    "    \"db_name\": \"test\",\n",
    "}\n",
    "\n",
    "ai_functions = OceanBaseAIFunctions(connection_args=connection_args)\n",
    "print(\"AI Functions client initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query AI Models and Endpoints\n",
    "\n",
    "You can query all configured AI models and endpoints to check your configuration.\n",
    "\n",
    "### List AI Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all configured AI models\n",
    "models = ai_functions.list_ai_models()\n",
    "\n",
    "print(f\"Found {len(models)} AI model(s):\\n\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"Model {i}:\")\n",
    "    print(f\"  Name: {model.get('model_name')}\")\n",
    "    print(f\"  Type: {model.get('type')} (1=embedding, 3=completion)\")\n",
    "    print(f\"  Model ID: {model.get('model_id')}\")\n",
    "    print(f\"  Created: {model.get('gmt_create')}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List AI Model Endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all configured AI model endpoints\n",
    "endpoints = ai_functions.list_ai_model_endpoints()\n",
    "\n",
    "print(f\"Found {len(endpoints)} AI model endpoint(s):\\n\")\n",
    "for i, endpoint in enumerate(endpoints, 1):\n",
    "    print(f\"Endpoint {i}:\")\n",
    "    print(f\"  Name: {endpoint.get('ENDPOINT_NAME')}\")\n",
    "    print(f\"  AI Model: {endpoint.get('AI_MODEL_NAME')}\")\n",
    "    print(f\"  URL: {endpoint.get('URL')}\")\n",
    "    print(f\"  Provider: {endpoint.get('PROVIDER')}\")\n",
    "    print(f\"  Scope: {endpoint.get('SCOPE')}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: AI functions are only supported in OceanBase 4.4.1+ or SeekDB. If you're using an older version, initialization will raise a `ValueError`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure Model Endpoints\n",
    "\n",
    "Set up API endpoints for each model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure embedding model endpoint\n",
    "ai_functions.create_ai_model_endpoint(\n",
    "    endpoint_name=\"embedding_endpoint\",\n",
    "    ai_model_name=\"your-embedding-model\",\n",
    "    url=\"https://api.example.com/v1\",\n",
    "    access_key=\"YOUR_API_KEY\",\n",
    "    provider=\"openai\"\n",
    ")\n",
    "\n",
    "# Configure completion model endpoint\n",
    "ai_functions.create_ai_model_endpoint(\n",
    "    endpoint_name=\"complete_endpoint\",\n",
    "    ai_model_name=\"your-completion-model\",\n",
    "    url=\"https://api.example.com/v1\",\n",
    "    access_key=\"YOUR_API_KEY\",\n",
    "    provider=\"openai\"\n",
    ")\n",
    "\n",
    "print(\"✅ Endpoints configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query Model Endpoints\n",
    "\n",
    "Verify that endpoints are configured:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all configured AI model endpoints\n",
    "endpoints = ai_functions.list_ai_model_endpoints()\n",
    "\n",
    "print(f\"Found {len(endpoints)} AI model endpoint(s):\\n\")\n",
    "for i, endpoint in enumerate(endpoints, 1):\n",
    "    print(f\"Endpoint {i}:\")\n",
    "    print(f\"  Name: {endpoint.get('ENDPOINT_NAME')}\")\n",
    "    print(f\"  AI Model: {endpoint.get('AI_MODEL_NAME')}\")\n",
    "    print(f\"  URL: {endpoint.get('URL')}\")\n",
    "    print(f\"  Provider: {endpoint.get('PROVIDER')}\")\n",
    "    print(f\"  Scope: {endpoint.get('SCOPE')}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Delete Models (Optional)\n",
    "\n",
    "If you need to remove models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete models (Note: delete endpoints first)\n",
    "# ai_functions.drop_ai_model(\"your-embedding-model\")\n",
    "# ai_functions.drop_ai_model(\"your-completion-model\")\n",
    "print(\"Note: Model deletion is commented out. Uncomment to delete models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Delete Model Endpoints\n",
    "\n",
    "Remove model endpoints:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model endpoints\n",
    "# ai_functions.drop_ai_model_endpoint(\"embedding_endpoint\")\n",
    "# ai_functions.drop_ai_model_endpoint(\"complete_endpoint\")\n",
    "print(\"Note: Endpoint deletion is commented out. Uncomment to delete endpoints.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test AI Functions\n",
    "\n",
    "After configuration, test each AI function to verify they work correctly.\n",
    "\n",
    "### Step 7: Test AI_COMPLETE\n",
    "\n",
    "Test text generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AI_COMPLETE\n",
    "completion = ai_functions.ai_complete(\n",
    "    prompt=\"Explain what machine learning is in one sentence\",\n",
    "    model_name=\"your-completion-model\"\n",
    ")\n",
    "print(f\"Completion: {completion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Test AI_EMBED\n",
    "\n",
    "Test text embedding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AI_EMBED\n",
    "vector = ai_functions.ai_embed(\n",
    "    text=\"Test text: Machine learning is a subset of artificial intelligence\",\n",
    "    model_name=\"your-embedding-model\"\n",
    ")\n",
    "print(f\"✅ Embedding successful: {len(vector)} dimensions\")\n",
    "print(f\"First 5 values: {vector[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Test AI_RERANK\n",
    "\n",
    "Test document reranking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AI_RERANK\n",
    "query = \"machine learning algorithms\"\n",
    "documents = [\n",
    "    \"Deep learning is a branch of machine learning that uses multi-layer neural networks\",\n",
    "    \"Python is a popular programming language widely used in data science\",\n",
    "    \"Supervised learning requires labeled data to train models\"\n",
    "]\n",
    "\n",
    "reranked = ai_functions.ai_rerank(\n",
    "    query=query,\n",
    "    documents=documents,\n",
    "    model_name=\"your-embedding-model\",\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(\"Reranked results:\")\n",
    "for result in reranked:\n",
    "    print(f\"Rank {result['rank']}: Score {result['score']:.4f}\")\n",
    "    print(f\"  Document: {result['document']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI_EMBED\n",
    "\n",
    "The `AI_EMBED` function converts text to vector embeddings, which can be used for semantic search and similarity matching.\n",
    "\n",
    "### Basic Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed text to vector\n",
    "text = \"Machine learning is a subset of artificial intelligence\"\n",
    "vector = ai_functions.ai_embed(text=text)\n",
    "print(f\"Embedding dimension: {len(vector)}\")\n",
    "print(f\"First 5 values: {vector[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Model Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify embedding model\n",
    "vector = ai_functions.ai_embed(\n",
    "    text=\"Hello, world!\",\n",
    "    model_name=\"your-embedding-model\"\n",
    ")\n",
    "print(f\"Embedding generated with model: {len(vector)} dimensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify embedding dimension\n",
    "vector = ai_functions.ai_embed(\n",
    "    text=\"Natural language processing\",\n",
    "    model_name=\"your-embedding-model\",\n",
    "    dimension=384\n",
    ")\n",
    "print(f\"Embedding with specified dimension: {len(vector)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI_COMPLETE\n",
    "\n",
    "The `AI_COMPLETE` function generates text completions using Large Language Models (LLMs).\n",
    "\n",
    "### Basic Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text completion\n",
    "prompt = \"What is machine learning?\"\n",
    "completion = ai_functions.ai_complete(prompt=prompt)\n",
    "print(f\"Completion: {completion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Model Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify LLM model\n",
    "completion = ai_functions.ai_complete(\n",
    "    prompt=\"Explain quantum computing in simple terms\",\n",
    "    model_name=\"text-generation-model\"\n",
    ")\n",
    "print(f\"Completion: {completion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Content Replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use template with {{TEXT}} placeholder\n",
    "prompt = \"Translate to English: {{TEXT}}\"\n",
    "completion = ai_functions.ai_complete(\n",
    "    prompt=prompt,\n",
    "    model_name=\"text-generation-model\",\n",
    "    content=\"Hello world\"\n",
    ")\n",
    "print(f\"Translation: {completion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize generation parameters\n",
    "options = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"presence_penalty\": 0.1\n",
    "}\n",
    "\n",
    "completion = ai_functions.ai_complete(\n",
    "    prompt=\"Write a short story about AI\",\n",
    "    model_name=\"text-generation-model\",\n",
    "    options=options\n",
    ")\n",
    "print(f\"Completion: {completion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI_RERANK\n",
    "\n",
    "The `AI_RERANK` function reranks search results to improve relevance by using semantic understanding.\n",
    "\n",
    "### Basic Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank documents\n",
    "query = \"machine learning algorithms\"\n",
    "documents = [\n",
    "    \"Deep learning uses neural networks for pattern recognition\",\n",
    "    \"Supervised learning requires labeled training data\",\n",
    "    \"Python is a popular programming language\",\n",
    "    \"Reinforcement learning learns through trial and error\",\n",
    "    \"Databases store structured information\"\n",
    "]\n",
    "\n",
    "reranked = ai_functions.ai_rerank(\n",
    "    query=query,\n",
    "    documents=documents,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"Reranked results:\")\n",
    "for result in reranked:\n",
    "    print(f\"Rank {result['rank']}: Score {result['score']:.4f}\")\n",
    "    print(f\"  Document: {result['document'][:50]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Model Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify reranking model\n",
    "reranked = ai_functions.ai_rerank(\n",
    "    query=\"artificial intelligence\",\n",
    "    documents=[\n",
    "        \"Machine learning enables computers to learn from data\",\n",
    "        \"Natural language processing understands human language\",\n",
    "        \"Computer vision interprets visual information\"\n",
    "    ],\n",
    "    model_name=\"rerank-model\",\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(\"Top 2 reranked results:\")\n",
    "for result in reranked:\n",
    "    print(f\"Rank {result['rank']}: {result['document']}\")\n",
    "    print(f\"  Score: {result['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerank All Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all reranked documents (no top_k limit)\n",
    "reranked = ai_functions.ai_rerank(\n",
    "    query=\"neural networks\",\n",
    "    documents=[\n",
    "        \"Convolutional neural networks excel at image recognition\",\n",
    "        \"Recurrent neural networks process sequential data\",\n",
    "        \"Transformers revolutionized NLP tasks\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"All reranked results:\")\n",
    "for result in reranked:\n",
    "    print(f\"Rank {result['rank']}: Score {result['score']:.4f}\")\n",
    "    print(f\"  {result['document']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Operations\n",
    "\n",
    "Process multiple texts efficiently using batch operations.\n",
    "\n",
    "### Batch Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed multiple texts at once\n",
    "texts = [\n",
    "    \"Machine learning algorithms\",\n",
    "    \"Deep learning neural networks\",\n",
    "    \"Natural language processing\",\n",
    "    \"Computer vision systems\"\n",
    "]\n",
    "\n",
    "vectors = ai_functions.batch_ai_embed(\n",
    "    texts=texts,\n",
    "    model_name=\"your-embedding-model\"\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(vectors)} embeddings\")\n",
    "print(f\"Each embedding has {len(vectors[0])} dimensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "### Use Case 1: Building a Semantic Search System\n",
    "\n",
    "Combine AI_EMBED with vector search for semantic search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Embed query\n",
    "query = \"How does neural network training work?\"\n",
    "query_vector = ai_functions.ai_embed(\n",
    "    text=query,\n",
    "    model_name=\"your-embedding-model\"\n",
    ")\n",
    "\n",
    "# Step 2: Use vector for similarity search\n",
    "# (This would typically be done with OceanbaseVectorStore)\n",
    "# vector_store.similarity_search_by_vector(query_vector, k=5)\n",
    "\n",
    "print(f\"Query vector dimension: {len(query_vector)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2: RAG with Reranking\n",
    "\n",
    "Improve RAG results by reranking retrieved documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Retrieve documents (example)\n",
    "retrieved_docs = [\n",
    "    \"Neural networks consist of layers of interconnected nodes\",\n",
    "    \"Training involves forward and backward propagation\",\n",
    "    \"Gradient descent optimizes network parameters\",\n",
    "    \"Python libraries like TensorFlow simplify implementation\"\n",
    "]\n",
    "\n",
    "# Step 2: Rerank for better relevance\n",
    "query = \"How to train a neural network?\"\n",
    "reranked = ai_functions.ai_rerank(\n",
    "    query=query,\n",
    "    documents=retrieved_docs,\n",
    "    model_name=\"rerank-model\",\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(\"Most relevant documents:\")\n",
    "for result in reranked:\n",
    "    print(f\"{result['document']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 3: Text Generation Pipeline\n",
    "\n",
    "Use AI_COMPLETE for content generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "documents = [\n",
    "    \"Machine learning is transforming industries by enabling computers to learn from data without explicit programming.\",\n",
    "    \"Deep learning enables breakthrough applications in image recognition, natural language processing, and autonomous systems.\",\n",
    "    \"AI is reshaping the future of technology, creating new possibilities and challenges.\"\n",
    "]\n",
    "\n",
    "for doc in documents:\n",
    "    prompt = f\"Summarize the following text in one sentence: {{TEXT}}\"\n",
    "    summary = ai_functions.ai_complete(\n",
    "        prompt=prompt,\n",
    "        model_name=\"text-generation-model\",\n",
    "        content=doc\n",
    "    )\n",
    "    print(f\"Summary: {summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 4: Multi-language Support\n",
    "\n",
    "Use AI_COMPLETE for translation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate text\n",
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Thank you for your help\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    prompt = \"Translate to Chinese: {{TEXT}}\"\n",
    "    translation = ai_functions.ai_complete(\n",
    "        prompt=prompt,\n",
    "        model_name=\"text-generation-model\",\n",
    "        content=text\n",
    "    )\n",
    "    print(f\"{text} -> {translation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "### Version Support\n",
    "- **OceanBase 4.4.1+**: Full support for all AI functions\n",
    "- **SeekDB**: Full support for all AI functions\n",
    "- **Automatic version checking**: Validates database version on initialization\n",
    "\n",
    "### Function Capabilities\n",
    "- **AI_EMBED**: Convert text to high-dimensional vector embeddings\n",
    "- **AI_COMPLETE**: Generate text using state-of-the-art LLMs\n",
    "- **AI_RERANK**: Improve search result relevance with semantic reranking\n",
    "\n",
    "### Error Handling\n",
    "- Graceful handling of missing model configurations\n",
    "- Clear error messages for unsupported database versions\n",
    "- Fallback mechanisms for batch operations\n",
    "\n",
    "### Performance\n",
    "- Efficient batch processing for multiple texts\n",
    "- Optimized SQL execution for AI function calls\n",
    "- Support for concurrent operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "For detailed documentation of all OceanBaseAIFunctions methods and parameters, see the API reference:\n",
    "\n",
    "### AI Functions\n",
    "- `ai_embed()`: Convert text to vector embeddings\n",
    "- `ai_complete()`: Generate text completions\n",
    "- `ai_rerank()`: Rerank documents by relevance\n",
    "- `batch_ai_embed()`: Batch process multiple texts\n",
    "\n",
    "### Model Configuration and Query\n",
    "- `create_ai_model()`: Create an AI model\n",
    "- `drop_ai_model()`: Drop an AI model\n",
    "- `create_ai_model_endpoint()`: Create a model endpoint\n",
    "- `alter_ai_model_endpoint()`: Alter a model endpoint\n",
    "- `drop_ai_model_endpoint()`: Drop a model endpoint\n",
    "- `list_ai_models()`: List all configured AI models\n",
    "- `list_ai_model_endpoints()`: List all configured AI model endpoints\n",
    "\n",
    "## References\n",
    "\n",
    "- [OceanBase AI Functions Documentation](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000004018305)\n",
    "- [OceanBase AI Functions Guide](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000004018306)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
