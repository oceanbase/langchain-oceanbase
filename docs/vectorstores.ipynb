{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1957f5cb",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Oceanbase\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f0986",
   "metadata": {},
   "source": [
    "# OceanbaseVectorStore\n",
    "\n",
    "This notebook covers how to get started with the Oceanbase vector store.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Setup](#setup) - Deploy OceanBase and install dependencies\n",
    "- [Initialization](#initialization) - Configure and create vector store\n",
    "- [Manage vector store](#manage-vector-store) - Add, update, and delete vectors\n",
    "- [Query vector store](#query-vector-store) - Search and retrieve vectors\n",
    "- [Build RAG (Retrieval Augmented Generation)](#build-rag-retrieval-augmented-generation) - Build powerful RAG applications\n",
    "- [Full-text Search](#full-text-search) - Implement full-text search capabilities\n",
    "- [Hybrid Search](#hybrid-search) - Combine vector and text search for better results\n",
    "- [Advanced Filtering](#advanced-filtering) - Metadata filtering and complex query conditions\n",
    "- [Maximal Marginal Relevance](#maximal-marginal-relevance) - Filter for diversity in search results\n",
    "- [Multiple Index Types](#multiple-index-types) - Different vector index types (HNSW, IVF, FLAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fdc060",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To access Oceanbase vector stores you'll need to deploy a standalone OceanBase server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b92118",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --name=oceanbase -e MODE=mini -e OB_SERVER_IP=127.0.0.1 -p 2881:2881 -d oceanbase/oceanbase-ce:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b687bdc",
   "metadata": {},
   "source": [
    "And install the `langchain-oceanbase` integration package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e28aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \"langchain-oceanbase\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea850342",
   "metadata": {},
   "source": [
    "Check the connection to OceanBase and set the memory usage ratio for vector data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066bbc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.CursorResult at 0x12696f2a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyobvector import ObVecClient\n",
    "\n",
    "tmp_client = ObVecClient()\n",
    "tmp_client.perform_raw_text_sql(\"ALTER SYSTEM ob_vector_memory_limit_percentage = 30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df377e",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Configure the API key of the embedded model. Here we use `DashScopeEmbeddings` as an example. When deploying `Oceanbase` with a Docker image as described above, simply follow the script below to set the `host`, `port`, `user`, `password`, and `database name`. For other deployment methods, set these parameters according to the actual situation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff29e3b7",
   "metadata": {},
   "source": [
    "%pip install dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc37144c-208d-4ab3-9f3a-0407a69fe052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "from langchain_oceanbase.vectorstores import OceanbaseVectorStore\n",
    "\n",
    "DASHSCOPE_API = os.environ.get(\"DASHSCOPE_API_KEY\", \"\")\n",
    "connection_args = {\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": \"2881\",\n",
    "    \"user\": \"root@test\",\n",
    "    \"password\": \"\",\n",
    "    \"db_name\": \"test\",\n",
    "}\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\", dashscope_api_key=DASHSCOPE_API\n",
    ")\n",
    "\n",
    "vector_store = OceanbaseVectorStore(\n",
    "    embedding_function=embeddings,\n",
    "    table_name=\"langchain_vector\",\n",
    "    connection_args=connection_args,\n",
    "    vidx_metric_type=\"l2\",\n",
    "    drop_old=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6071d4",
   "metadata": {},
   "source": [
    "## Manage vector store\n",
    "\n",
    "### Add items to vector store\n",
    "\n",
    "- TODO: Edit and then run code cell to generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17f5efc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"foo\",\n",
    "    metadata={\"source\": \"https://foo.com\"}\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"bar\",\n",
    "    metadata={\"source\": \"https://bar.com\"}\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"baz\",\n",
    "    metadata={\"source\": \"https://baz.com\"}\n",
    ")\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "\n",
    "vector_store.add_documents(documents=documents,ids=[\"1\",\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738c3e0",
   "metadata": {},
   "source": [
    "### Update items in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0aa8b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_document = Document(\n",
    "    page_content=\"qux\",\n",
    "    metadata={\"source\": \"https://another-example.com\"}\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=[updated_document],ids=[\"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1b905",
   "metadata": {},
   "source": [
    "### Delete items from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef61e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete(ids=[\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3620501",
   "metadata": {},
   "source": [
    "## Query vector store\n",
    "\n",
    "Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. \n",
    "\n",
    "### Query directly\n",
    "\n",
    "Performing a simple similarity search can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa0a16fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* bar [{'source': 'https://bar.com'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query=\"thud\",k=1,filter={\"source\":\"https://another-example.com\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9d733",
   "metadata": {},
   "source": [
    "If you want to execute a similarity search and receive the corresponding scores you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5efd2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=133.452299] bar [{'source': 'https://bar.com'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query=\"thud\",k=1,filter={\"source\":\"https://example.com\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c235cdc",
   "metadata": {},
   "source": [
    "### Query by turning into retriever\n",
    "\n",
    "You can also transform the vector store into a retriever for easier usage in your chains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3460093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://bar.com'}, page_content='bar')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")\n",
    "retriever.invoke(\"thud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c75dc",
   "metadata": {},
   "source": [
    "## Usage for retrieval-augmented generation\n",
    "\n",
    "For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:\n",
    "\n",
    "- [Tutorials](/docs/tutorials/)\n",
    "- [How-to: Question and answer with RAG](https://python.langchain.com/docs/how_to/#qa-with-rag)\n",
    "- [Retrieval conceptual docs](https://python.langchain.com/docs/concepts/#retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e72db0",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "This section demonstrates the advanced features of OceanbaseVectorStore:\n",
    "\n",
    "*   **Vector Storage**: Store embeddings from any LangChain embedding model in OceanBase with automatic table creation and index management.\n",
    "*   **Similarity Search**: Perform efficient similarity searches on vector data with multiple distance metrics (L2, cosine, inner product).\n",
    "*   **Hybrid Search**: Combine vector search with sparse vector search and full-text search for improved results with configurable weights.\n",
    "*   **Maximal Marginal Relevance**: Filter for diversity in search results to avoid redundant information.\n",
    "*   **Multiple Index Types**: Support for HNSW, IVF, FLAT and other vector index types with automatic parameter optimization.\n",
    "*   **Sparse Embeddings**: Native support for sparse vector embeddings with BM25-like functionality.\n",
    "*   **Advanced Filtering**: Built-in support for metadata filtering and complex query conditions.\n",
    "*   **Async Support**: Full support for async operations and high-concurrency scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c57297",
   "metadata": {},
   "source": [
    "### Build RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Discover how to build powerful RAG applications by combining LangChain with OceanBase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3c7b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Create vector store for knowledge base\n",
    "rag_vectorstore = OceanbaseVectorStore(\n",
    "    embedding_function=embeddings,\n",
    "    table_name=\"knowledge_base\",\n",
    "    connection_args=connection_args,\n",
    "    vidx_metric_type=\"l2\",\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "# Add knowledge documents\n",
    "documents = [\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"OceanBase is a distributed relational database developed by Ant Group.\",\n",
    "    \"Vector databases are specialized databases for storing and searching vector embeddings.\",\n",
    "    \"Hybrid search combines multiple search modalities for better results.\"\n",
    "]\n",
    "rag_vectorstore.add_texts(documents)\n",
    "\n",
    "# Build RAG chain\n",
    "retriever = rag_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "question = \"What is LangChain?\"\n",
    "answer = qa_chain.run(question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bdfed4",
   "metadata": {},
   "source": [
    "### Full-text Search\n",
    "\n",
    "Explore how to implement full-text search capabilities using LangChain and OceanBase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store with full-text search enabled\n",
    "fulltext_vectorstore = OceanbaseVectorStore(\n",
    "    embedding_function=embeddings,\n",
    "    table_name=\"fulltext_docs\",\n",
    "    connection_args=connection_args,\n",
    "    include_fulltext=True,\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "# Add documents with full-text content\n",
    "fulltext_documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language with dynamic semantics\",\n",
    "        metadata={\"category\": \"programming\", \"language\": \"python\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning algorithms can learn patterns from data automatically\",\n",
    "        metadata={\"category\": \"AI\", \"language\": \"general\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"OceanBase provides high availability and linear scalability\",\n",
    "        metadata={\"category\": \"database\", \"language\": \"general\"}\n",
    "    )\n",
    "]\n",
    "fulltext_vectorstore.add_documents_with_fulltext(fulltext_documents)\n",
    "\n",
    "# Perform full-text search\n",
    "fulltext_results = fulltext_vectorstore.similarity_search_with_fulltext(\n",
    "    query=\"programming language\", \n",
    "    k=2\n",
    ")\n",
    "print(\"Full-text search results:\")\n",
    "for doc in fulltext_results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdfcb2",
   "metadata": {},
   "source": [
    "### Hybrid Search\n",
    "\n",
    "Learn how to combine vector and keyword search for more accurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store with hybrid search capabilities\n",
    "hybrid_vectorstore = OceanbaseVectorStore(\n",
    "    embedding_function=embeddings,\n",
    "    table_name=\"hybrid_search_docs\",\n",
    "    connection_args=connection_args,\n",
    "    include_sparse=True,\n",
    "    include_fulltext=True,\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "# Add documents with sparse vectors\n",
    "hybrid_documents = [\n",
    "    Document(\n",
    "        page_content=\"Artificial intelligence and machine learning are transforming industries worldwide\",\n",
    "        metadata={\"sparse_vector\": {1: 0.8, 3: 0.6, 5: 0.4}, \"category\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep learning neural networks require large amounts of training data\",\n",
    "        metadata={\"sparse_vector\": {2: 0.7, 4: 0.5, 6: 0.3}, \"category\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"OceanBase database provides excellent performance for vector operations\",\n",
    "        metadata={\"sparse_vector\": {1: 0.3, 7: 0.9, 8: 0.6}, \"category\": \"database\"}\n",
    "    )\n",
    "]\n",
    "hybrid_vectorstore.add_documents(hybrid_documents)\n",
    "\n",
    "# Advanced hybrid search with custom weights\n",
    "hybrid_results = hybrid_vectorstore.advanced_hybrid_search(\n",
    "    vector_query=\"AI and machine learning\",\n",
    "    sparse_query={1: 0.5, 3: 0.8, 5: 0.2},\n",
    "    fulltext_query=\"artificial intelligence\",\n",
    "    modality_weights={\n",
    "        \"vector\": 0.4,\n",
    "        \"sparse\": 0.3,\n",
    "        \"fulltext\": 0.3\n",
    "    },\n",
    "    k=3\n",
    ")\n",
    "print(\"Hybrid search results:\")\n",
    "for doc in hybrid_results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a04a67",
   "metadata": {},
   "source": [
    "### Advanced Filtering\n",
    "\n",
    "Use metadata filtering for precise document retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata filters\n",
    "filtered_results = hybrid_vectorstore.similarity_search_with_advanced_filters(\n",
    "    query=\"machine learning\",\n",
    "    k=3,\n",
    "    filter_conditions={\n",
    "        \"category\": \"AI\"\n",
    "    }\n",
    ")\n",
    "print(\"Filtered results (AI category only):\")\n",
    "for doc in filtered_results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e50e16",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance\n",
    "\n",
    "Filter for diversity in search results to avoid redundant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaec2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MMR to get diverse results\n",
    "diverse_results = hybrid_vectorstore.max_marginal_relevance_search(\n",
    "    query=\"artificial intelligence\",\n",
    "    k=3,\n",
    "    fetch_k=10,\n",
    "    lambda_mult=0.5\n",
    ")\n",
    "print(\"Diverse results using MMR:\")\n",
    "for doc in diverse_results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6f6b0",
   "metadata": {},
   "source": [
    "### Multiple Index Types\n",
    "\n",
    "OceanbaseVectorStore supports various vector index types for different use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with different index types\n",
    "index_types = [\"HNSW\", \"IVF\", \"FLAT\"]\n",
    "\n",
    "for index_type in index_types:\n",
    "    print(f\"\\n=== Testing {index_type} Index ===\")\n",
    "    \n",
    "    # Create vector store with specific index type\n",
    "    test_vectorstore = OceanbaseVectorStore(\n",
    "        embedding_function=embeddings,\n",
    "        table_name=f\"test_{index_type.lower()}\",\n",
    "        connection_args=connection_args,\n",
    "        vidx_metric_type=\"l2\",\n",
    "        index_type=index_type,\n",
    "        drop_old=True,\n",
    "    )\n",
    "    \n",
    "    # Add test documents\n",
    "    test_docs = [\n",
    "        \"This is a test document for index performance comparison\",\n",
    "        \"Vector search performance varies with different index types\",\n",
    "        \"HNSW is good for high-dimensional vectors, IVF for large datasets\"\n",
    "    ]\n",
    "    test_vectorstore.add_texts(test_docs)\n",
    "    \n",
    "    # Test search performance\n",
    "    results = test_vectorstore.similarity_search(\"vector search\", k=2)\n",
    "    print(f\"Search results with {index_type}:\")\n",
    "    for doc in results:\n",
    "        print(f\"- {doc.page_content}\")\n",
    "    \n",
    "    # Clean up\n",
    "    test_vectorstore.delete_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27244f",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all OceanbaseVectorStore features and configurations head to the API reference: https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/integrations/vectorstores/oceanbase.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
