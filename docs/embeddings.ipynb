{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: Embeddings\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Built-in Embedding Function Guide\n",
        "\n",
        "This notebook demonstrates how to use the built-in embedding functionality provided by `langchain-oceanbase`. The built-in embedding uses the `all-MiniLM-L6-v2` model (384 dimensions) and requires no external API keys, making it perfect for quick prototyping and testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "The built-in embedding functionality requires the `pyseekdb` package, which uses ONNX runtime for local inference:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU \"langchain-oceanbase\" \"pyseekdb\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Direct Use of DefaultEmbeddingFunction\n",
        "\n",
        "`DefaultEmbeddingFunction` is the default embedding function provided by `pyseekdb` and can be used directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_oceanbase import DefaultEmbeddingFunction\n",
        "\n",
        "# Create embedding function\n",
        "ef = DefaultEmbeddingFunction()\n",
        "print(f\"Embedding dimension: {ef.dimension}\")  # 384\n",
        "\n",
        "# Generate embedding for a single text\n",
        "single_text = \"Hello world\"\n",
        "single_embedding = ef(single_text)\n",
        "print(f\"Single text embedding count: {len(single_embedding)}\")\n",
        "print(f\"Single text embedding dimension: {len(single_embedding[0]) if single_embedding else 0}\")\n",
        "\n",
        "# Generate embeddings for multiple texts (batch processing)\n",
        "texts = [\"Hello world\", \"How are you?\", \"Python programming\", \"Machine learning\"]\n",
        "embeddings = ef(texts)\n",
        "print(f\"\\nBatch text count: {len(texts)}\")\n",
        "print(f\"Generated embedding count: {len(embeddings)}\")\n",
        "print(f\"Each embedding dimension: {len(embeddings[0]) if embeddings else 0}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 2: Using DefaultEmbeddingFunctionAdapter (LangChain Compatible Interface)\n",
        "\n",
        "`DefaultEmbeddingFunctionAdapter` implements LangChain's `Embeddings` interface, allowing seamless integration into the LangChain ecosystem:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_oceanbase.embedding_utils import DefaultEmbeddingFunctionAdapter\n",
        "\n",
        "# Create adapter\n",
        "adapter = DefaultEmbeddingFunctionAdapter()\n",
        "print(f\"Embedding dimension: {adapter.dimension}\")\n",
        "\n",
        "# Generate embeddings for multiple documents (batch)\n",
        "documents = [\"Hello world\", \"How are you?\", \"Python programming\"]\n",
        "doc_embeddings = adapter.embed_documents(documents)\n",
        "print(f\"\\nDocument count: {len(documents)}\")\n",
        "print(f\"Generated document embedding count: {len(doc_embeddings)}\")\n",
        "print(f\"Each document embedding dimension: {len(doc_embeddings[0]) if doc_embeddings else 0}\")\n",
        "\n",
        "# Generate embedding for a query (single text)\n",
        "query = \"Hello\"\n",
        "query_embedding = adapter.embed_query(query)\n",
        "print(f\"\\nQuery text: '{query}'\")\n",
        "print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "print(f\"Query embedding type: {type(query_embedding)}\")  # Should be a 1D list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 3: Using Default Embedding in OceanbaseVectorStore\n",
        "\n",
        "The simplest way is to set `embedding_function=None` when creating `OceanbaseVectorStore`, and the system will automatically use the default embedding function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_oceanbase.vectorstores import OceanbaseVectorStore\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Connection configuration\n",
        "connection_args = {\n",
        "    \"host\": \"127.0.0.1\",\n",
        "    \"port\": \"2881\",\n",
        "    \"user\": \"root@test\",\n",
        "    \"password\": \"\",\n",
        "    \"db_name\": \"test\",\n",
        "}\n",
        "\n",
        "# Use default embedding (set embedding_function=None)\n",
        "vector_store = OceanbaseVectorStore(\n",
        "    embedding_function=None,  # Automatically uses DefaultEmbeddingFunction\n",
        "    table_name=\"embedding_demo\",\n",
        "    connection_args=connection_args,\n",
        "    vidx_metric_type=\"l2\",\n",
        "    drop_old=True,\n",
        "    embedding_dim=384,  # all-MiniLM-L6-v2 dimension\n",
        ")\n",
        "\n",
        "print(\"✓ Vector store created successfully, using default embedding\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding Documents and Performing Search\n",
        "\n",
        "Add documents and perform similarity search using the default embedding:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add documents\n",
        "documents = [\n",
        "    Document(page_content=\"Machine learning is a subset of artificial intelligence\"),\n",
        "    Document(page_content=\"Python is a popular programming language\"),\n",
        "    Document(page_content=\"OceanBase is a distributed relational database\"),\n",
        "    Document(page_content=\"Deep learning uses neural networks for pattern recognition\"),\n",
        "]\n",
        "\n",
        "ids = vector_store.add_documents(documents)\n",
        "print(f\"✓ Successfully added {len(ids)} documents\")\n",
        "\n",
        "# Perform similarity search\n",
        "results = vector_store.similarity_search(\"artificial intelligence\", k=2)\n",
        "print(f\"\\nSimilarity search for 'artificial intelligence' returned {len(results)} results:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"{i}. {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Text Similarity\n",
        "\n",
        "Use embeddings to compute similarity between texts:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_oceanbase.embedding_utils import DefaultEmbeddingFunctionAdapter\n",
        "import math\n",
        "\n",
        "# Create adapter\n",
        "adapter = DefaultEmbeddingFunctionAdapter()\n",
        "\n",
        "# Define similar and dissimilar texts\n",
        "text1 = \"Machine learning is a subset of artificial intelligence\"\n",
        "text2 = \"AI and machine learning are related technologies\"\n",
        "text3 = \"The weather today is sunny and warm\"\n",
        "\n",
        "# Generate embeddings\n",
        "emb1 = adapter.embed_query(text1)\n",
        "emb2 = adapter.embed_query(text2)\n",
        "emb3 = adapter.embed_query(text3)\n",
        "\n",
        "# Compute cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
        "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
        "    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
        "    norm2 = math.sqrt(sum(b * b for b in vec2))\n",
        "    return dot_product / (norm1 * norm2) if norm1 * norm2 > 0 else 0.0\n",
        "\n",
        "sim_12 = cosine_similarity(emb1, emb2)\n",
        "sim_13 = cosine_similarity(emb1, emb3)\n",
        "\n",
        "print(f\"Text 1: '{text1[:50]}...'\")\n",
        "print(f\"Text 2: '{text2[:50]}...'\")\n",
        "print(f\"Text 3: '{text3[:50]}...'\")\n",
        "print(f\"\\nSimilarity between Text 1 and Text 2: {sim_12:.4f}\")\n",
        "print(f\"Similarity between Text 1 and Text 3: {sim_13:.4f}\")\n",
        "print(f\"\\n✓ Related text similarity ({sim_12:.4f}) > Unrelated text similarity ({sim_13:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage: Custom Model and Providers\n",
        "\n",
        "`DefaultEmbeddingFunctionAdapter` supports custom model names and ONNX runtime providers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use default configuration\n",
        "adapter_default = DefaultEmbeddingFunctionAdapter()\n",
        "print(f\"Default configuration - Dimension: {adapter_default.dimension}\")\n",
        "\n",
        "# Custom model name (currently only 'all-MiniLM-L6-v2' is supported)\n",
        "adapter_custom = DefaultEmbeddingFunctionAdapter(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    preferred_providers=[\"CPUExecutionProvider\"]  # Specify CPU usage\n",
        ")\n",
        "print(f\"Custom configuration - Dimension: {adapter_custom.dimension}\")\n",
        "\n",
        "# Test embedding generation\n",
        "texts = [\"Test embedding generation\"]\n",
        "embeddings_default = adapter_default.embed_documents(texts)\n",
        "embeddings_custom = adapter_custom.embed_documents(texts)\n",
        "\n",
        "print(f\"\\nDefault configuration embedding dimension: {len(embeddings_default[0])}\")\n",
        "print(f\"Custom configuration embedding dimension: {len(embeddings_custom[0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison: Batch Processing vs Single Processing\n",
        "\n",
        "Demonstrate the performance advantage of batch processing over single processing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from langchain_oceanbase.embedding_utils import DefaultEmbeddingFunctionAdapter\n",
        "\n",
        "adapter = DefaultEmbeddingFunctionAdapter()\n",
        "texts = [\"Text \" + str(i) for i in range(100)]  # 100 texts\n",
        "\n",
        "# Method 1: Batch processing\n",
        "start_time = time.time()\n",
        "batch_embeddings = adapter.embed_documents(texts)\n",
        "batch_time = time.time() - start_time\n",
        "print(f\"Batch processing {len(texts)} texts took: {batch_time:.4f} seconds\")\n",
        "print(f\"Average per text: {batch_time/len(texts)*1000:.2f} milliseconds\")\n",
        "\n",
        "# Method 2: Single processing (not recommended)\n",
        "start_time = time.time()\n",
        "single_embeddings = [adapter.embed_query(text) for text in texts]\n",
        "single_time = time.time() - start_time\n",
        "print(f\"\\nSingle processing {len(texts)} texts took: {single_time:.4f} seconds\")\n",
        "print(f\"Average per text: {single_time/len(texts)*1000:.2f} milliseconds\")\n",
        "\n",
        "print(f\"\\n✓ Batch processing is {single_time/batch_time:.2f}x faster than single processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Example: Building a RAG Application\n",
        "\n",
        "Build a complete RAG (Retrieval Augmented Generation) application using the default embedding:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_oceanbase.vectorstores import OceanbaseVectorStore\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Create vector store using default embedding\n",
        "rag_vector_store = OceanbaseVectorStore(\n",
        "    embedding_function=None,  # Use default embedding\n",
        "    table_name=\"rag_knowledge_base\",\n",
        "    connection_args=connection_args,\n",
        "    vidx_metric_type=\"l2\",\n",
        "    drop_old=True,\n",
        "    embedding_dim=384,\n",
        ")\n",
        "\n",
        "# Add knowledge base documents\n",
        "knowledge_docs = [\n",
        "    Document(\n",
        "        page_content=\"LangChain is a framework for developing applications powered by language models\",\n",
        "        metadata={\"source\": \"langchain_docs\", \"topic\": \"framework\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"OceanBase is a distributed relational database developed by Ant Group\",\n",
        "        metadata={\"source\": \"oceanbase_docs\", \"topic\": \"database\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Vector databases are specialized databases for storing and searching vector embeddings\",\n",
        "        metadata={\"source\": \"vector_db_docs\", \"topic\": \"database\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Hybrid search combines multiple search modalities for better results\",\n",
        "        metadata={\"source\": \"search_docs\", \"topic\": \"search\"}\n",
        "    ),\n",
        "]\n",
        "\n",
        "ids = rag_vector_store.add_documents(knowledge_docs)\n",
        "print(f\"✓ Knowledge base created with {len(ids)} documents\")\n",
        "\n",
        "# Create retriever\n",
        "retriever = rag_vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Perform retrieval\n",
        "query = \"What is LangChain?\"\n",
        "results = retriever.invoke(query)\n",
        "print(f\"\\nQuery: '{query}'\")\n",
        "print(f\"Retrieved {len(results)} relevant documents:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n{i}. {doc.page_content}\")\n",
        "    print(f\"   Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Topic: {doc.metadata.get('topic', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Features Summary\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **No API Keys Required**: Uses local ONNX models, no external API calls needed\n",
        "2. **Quick Start**: Perfect for rapid prototyping and testing\n",
        "3. **LangChain Compatible**: Fully compatible with LangChain's `Embeddings` interface\n",
        "4. **Batch Processing**: Supports efficient batch embedding generation\n",
        "5. **Automatic Integration**: Can be automatically used in `OceanbaseVectorStore`\n",
        "\n",
        "### Technical Specifications\n",
        "\n",
        "- **Model**: all-MiniLM-L6-v2\n",
        "- **Dimension**: 384\n",
        "- **Inference Engine**: ONNX Runtime\n",
        "- **Supported Platforms**: CPU (default), optional GPU\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- Rapid prototyping\n",
        "- Local development and testing\n",
        "- Scenarios that don't require high-precision embeddings\n",
        "- Applications that need to run offline\n",
        "- Cost-sensitive applications\n",
        "\n",
        "### Notes\n",
        "\n",
        "- Requires `pyseekdb` package to be installed\n",
        "- Model files will be downloaded on first use (approximately 80MB)\n",
        "- For production environments, consider using more powerful embedding models (e.g., OpenAI, DashScope, etc.)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
